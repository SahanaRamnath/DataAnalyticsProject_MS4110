{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T17:41:50.842864Z",
     "start_time": "2018-11-11T17:41:46.675743Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfiles=['''enter test filenames in this order: comments_employee_mapping, comments_likeability, employee_attrition, happiness_level.csv''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T17:11:27.704482Z",
     "start_time": "2018-11-11T17:11:26.285768Z"
    }
   },
   "outputs": [],
   "source": [
    "mapping=pd.read_csv('comments_employee_mapping.csv').dropna()\n",
    "likes=pd.read_csv('comments_likeability.csv').dropna()\n",
    "attr=pd.read_csv('employee_attrition.csv').dropna()\n",
    "hap=pd.read_csv('happiness_level.csv').dropna()\n",
    "\n",
    "mapping_test=pd.read_csv(testfiles[0]).dropna()\n",
    "likes_test=pd.read_csv(testfiles[1]).dropna()\n",
    "attr_test=pd.read_csv(testfiles[2]).dropna()\n",
    "hap_test=pd.read_csv(testfiles[3]).dropna()\n",
    "\n",
    "attr['lastParticipationDate']=pd.to_datetime(attr['lastParticipationDate'],infer_datetime_format=True)\n",
    "attr_test['lastParticipationDate']=pd.to_datetime(attr_test['lastParticipationDate'],infer_datetime_format=True)\n",
    "attr['ordinal']=[x.toordinal() for x in attr['lastParticipationDate']]\n",
    "attr_test['ordinal']=[x.toordinal() for x in attr_test['lastParticipationDate']]\n",
    "\n",
    "for d in [mapping,likes,attr,hap,mapping_test,likes_test,attr_test,hap_test]:\n",
    "    d['id']=d['employee'].map(str)+d['companyAlias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Preproc\n",
    "id_dict,comm_dict={},{}\n",
    "for i in range(len(likes)):\n",
    "    try: id_dict[likes['id'][i]]==0\n",
    "    except: id_dict[likes['id'][i]]=len(id_dict.keys())\n",
    "    try: comm_dict[likes['commentId'][i]]==0\n",
    "    except: comm_dict[likes['commentId'][i]]=len(comm_dict.keys())\n",
    "\n",
    "m=np.zeros([len(id_dict.keys()),len(comm_dict.keys())])\n",
    "for i in range(len(likes)):\n",
    "    m[id_dict[likes['id'][i]],comm_dict[likes['commentId'][i]]]= likes['liked'][i]*1 - likes['disliked'][i]*1\n",
    "\n",
    "l=[]\n",
    "exc=0\n",
    "for i in range(len(attr)):\n",
    "    try:\n",
    "        l.append(m[id_dict[attr['id'][i]],:])\n",
    "    except:\n",
    "        exc+=1\n",
    "        l.append(np.zeros(len(comm_dict)))\n",
    "l=np.vstack(l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T17:20:41.151441Z",
     "start_time": "2018-11-11T17:20:41.095117Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test PreProc\n",
    "id_dict_test={}\n",
    "for i in range(len(likes_test)):\n",
    "    try: id_dict_test[likes_test['id'][i]]==0\n",
    "    except: id_dict_test[likes_test['id'][i]]=len(id_dict_test)\n",
    "\n",
    "m_test=np.zeros([len(id_dict_test.keys()),len(comm_dict.keys())])\n",
    "for i in range(len(likes_test)):\n",
    "    try:m_test[id_dict_test[likes_test['id'][i]],comm_dict[likes_test['commentId'][i]]]= likes_test['liked'][i]*1 - likes_test['disliked'][i]*1\n",
    "    except:pass\n",
    "\n",
    "l_test=[]\n",
    "for i in range(len(attr_test)):\n",
    "    try:l_test.append(m[id_dict_test[attr_test['id'][i]],:])\n",
    "    except:l_test.append(np.zeros(len(comm_dict)))\n",
    "l_test=np.vstack(l_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=pd.concat([attr.companyAlias,attr_test.companyAlias],axis=0)\n",
    "tmp2=pd.concat([attr.ordinal,attr_test.ordinal],axis=0)\n",
    "tmp3=np.vstack([l,l_test])\n",
    "\n",
    "data=np.hstack([tmp3,pd.get_dummies(tmp),((tmp2-tmp2.mean())/tmp2.std())[:,np.newaxis]])\n",
    "\n",
    "X_train=data[:len(attr),:]\n",
    "X_test=data[len(attr):,:]\n",
    "y_train=attr.stillExists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T17:19:42.546192Z",
     "start_time": "2018-11-11T17:19:42.493446Z"
    }
   },
   "outputs": [],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print 'training set accuracy:{}'.format(clf.score(X_train,y_train))\n",
    "pd.DataFrame(clf.predict(X_test)).to_csv('predictions.csv')\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
